data <- rast(paste0(odir,'ABGB_4x5_Xu2021.tif'))
ext(data)
data
plot(data[[10]])
# check if na exist
is.nan(data)
sum(is.nan(data))
# rename layers to insert month info
time_data <- format(seq(as.Date("2000-10-01"), as.Date("2019-12-31"), by = "year"), format = "%Y-%m")
names(data_45)<-time_data
# create 240 layers with -9999
out.data_45   <- rast(ncols=72, nrows=46,xmin=-180, xmax=180, ymin=-90, ymax=90, nlyrs=12, vals=-9999)
ext(out.data_45)
# match CARDAMOM-MAP convension for lon lat time dimension, resolution, and extent
rast_mask<-rast('/Users/shuangma/CARDAMOM-MAPS-DATASETS/Mask/CARDAMOM-MAPS_GC4x5_LAND_SEA_MASK.nc',subds='data')
year=2000
for (iyr in 1:length(2000:2019)){
for (imon in 1:12){
out.data_45[[imon]] <- data[[iyr]]
}
names(out.data_45)    <- seq(1, 12, by = 1)
CARDAMOM.out.data_45 <- trans(out.data_45)
CARDAMOM.out.data_45 <- flip(CARDAMOM.out.data_45, direction = "vertical")
CARDAMOM.out.data_45 <- flip(CARDAMOM.out.data_45, direction = "horizontal")
ext(CARDAMOM.out.data_45) <- ext(rast_mask)
writeCDF(CARDAMOM.out.data_45, paste0(odir, "/CARDAMOM-MAPS_GC4X5_Xu2021ABGB_", year, ".nc"), compression = 4,varname = "ABGB", unit = "gC/m2",
longname = "Xu2021_ABGB_GC4X5", zname = "time", overwrite = TRUE)
year=year+1
}
# t <- trans(out.data_45)
# f <- flip(t, direction = "vertical")
# f2 <- flip(f, direction = "horizontal")
f2
dim(f2)
dims(f2)
f2$dims
f2$dim
f2$ndim
f2$ndims
# t <- trans(out.data_45)
# f <- flip(t, direction = "vertical")
# f2 <- flip(f, direction = "horizontal")
dim(f2) <- c(46, 72, 12)
dim(f2)
plot(f2[[1]])
f2
# t <- trans(out.data_45)
# f <- flip(t, direction = "vertical")
f2 <- flip(f, direction = "horizontal")
f2
plot(f2[[1]])
data
rast_mask
plot(rast_mask)
f2
rast_mask
out.data_45
.
nc_mask <- nc_open('/Users/shuangma/CARDAMOM-MAPS-DATASETS/Mask/CARDAMOM-MAPS_GC4x5_LAND_SEA_MASK.nc')
nc_mask
nc_data <- nc_open(paste0(idir,'CARDAMOM-MAPS_05deg_2D_TEMPLATE.nc'))
nc_data
idir<-'/Users/shuangma/CARDAMOM-MAPS/DATA/'
odir<-'/Users/shuangma/RESEARCH/WORKFLOW/A1_ToolBox/A1_Regridding/CARDAMOM_regrid/CMS_FLUX/output/'
nc_data <- nc_open(paste0(idir,'CARDAMOM-MAPS_05deg_2D_TEMPLATE.nc'))
nc_data <- nc_open(paste0(idir,'CARDAMOM-MAPS_05deg_3D_TEMPLATE.nc'))
nc_data
year=2018
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, ".nc"))
nc_nbe
plot(out.data_45[[1]])
out.data_45
rast_mask
plot(rast_mask)
# read tif from matlab and save to nc in separate years
odir='/Users/shuangma/RESEARCH/WORKFLOW/A1_ToolBox/A1_Regridding/CARDAMOM_regrid/Xu2021/output/';
data <- rast(paste0(odir,'ABGB_4x5_Xu2021.tif'))
ext(data)
data
plot(data[[10]])
nc_mask <- nc_open('/Users/shuangma/CARDAMOM-MAPS-DATASETS/Mask/CARDAMOM-MAPS_GC4x5_LAND_SEA_MASK.nc')
nc_mask
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, ".nc"))
idir<-'/Users/shuangma/CARDAMOM-MAPS/DATA/'
odir<-'/Users/shuangma/RESEARCH/WORKFLOW/A1_ToolBox/A1_Regridding/CARDAMOM_regrid/CMS_FLUX/output/'
nc_data <- nc_open(paste0(idir,'CARDAMOM-MAPS_05deg_2D_TEMPLATE.nc'))
nc_data <- nc_open(paste0(idir,'CARDAMOM-MAPS_05deg_3D_TEMPLATE.nc'))
nc_met <- nc_open('/Users/shuangma/CARDAMOM-MAPS-DATASETS-EXAMPLE/MetDrivers/T2M_MIN/CARDAMOM-MAPS_05deg_ERA5_T2M_MIN_2001.nc')
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, ".nc"))
nc_data
nc_met
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, ".nc"))
nc_nbe
nc_nco_file <- nc_open(paste0(odir, "/outfile.nc"))
nc_nco_file
nc_nbe
nc_nco_file <- nc_open(paste0(odir, "/outfile.nc"))
nc_nco_file
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, "_swapped.nc"))
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, ".nc"))
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, "_swapped.nc"))
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_2010_swapped.nc"))
nc_nbe
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_2010_swapped.nc"))
nc_nbe
rast_nbe<-rast(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_2010_swapped.nc"),subds='data')
plot(rast_nbe)
rast_mask<-rast('/Users/shuangma/CARDAMOM-MAPS-DATASETS/Mask/CARDAMOM-MAPS_GC4x5_LAND_SEA_MASK.nc',subds='data')
plot(rast_mask)
nc_nbe <- nc_open(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_", year, ".nc"))
plot(nc_nbe)
rast_nbe<-rast(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_2010.nc"),subds='data')
rast_nbe_swapped<-rast(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_2010_swapped.nc"),subds='data')
plot(rast_nbe)
plot(rast_nbe_swapped)
plot(rast_nbe_swapped[[1]])
rast_mask<-rast('/Users/shuangma/CARDAMOM-MAPS-DATASETS/Mask/CARDAMOM-MAPS_GC4x5_LAND_SEA_MASK.nc',subds='data')
plot(rast_mask)
rast_nbe<-rast(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_2010.nc"),subds='data')
rast_nbe_swapped<-rast(paste0(odir, "/CARDAMOM-MAPS_GC4x5_CMSNBEUNC_2010_swapped.nc"),subds='data')
plot(rast_nbe[[1]])
plot(rast_nbe_swapped[[1]])
rast_mask<-rast('/Users/shuangma/CARDAMOM-MAPS-DATASETS/Mask/CARDAMOM-MAPS_GC4x5_LAND_SEA_MASK.nc',subds='data')
plot(rast_mask)
nc_T2M_MAX <- nc_open('/Users/shuangma/CARDAMOM-MAPS-DATASETS/MetDrivers/T2M_MAX/CARDAMOM-MAPS_GC4x5_ERA5_T2M_MAX_2008.nc')
nc_T2M_MAX
nc_met <- nc_open('/Users/shuangma/CARDAMOM-MAPS-DATASETS-EXAMPLE/MetDrivers/T2M_MIN/CARDAMOM-MAPS_05deg_ERA5_T2M_MIN_2001.nc')
nc_met
rast_mask<-rast('/Users/shuangma/CARDAMOM-MAPS-DATASETS/Mask/CARDAMOM-MAPS_GC4x5_LAND_SEA_MASK.nc',subds='data')
rast_mask
plot(rast_mask)
options(stringsAsFactors = FALSE)
library(terra)
library(ncdf4)
library(ncdf4.helpers)
# read  mean NBE data
idir<-'/Users/shuangma/RESEARCH/DATA/CMS_Flux/NBE_2020/'
odir<-'/Users/shuangma/RESEARCH/WORKFLOW/A1_ToolBox/A1_Regridding/CARDAMOM_regrid/CMS_FLUX/output/'
data45 <- rast(paste0(idir,'CMS-Flux.monthly.grid.2010-2018.nc'),
subds='post-NBE')
data45
max(data45)
plot(data45(,,1))
plot(data45[[1]]
)
plot(data45)
#  units are mgC/m2/day for CH4, g for c02
library(stringr)
library(tidyr)
library(dplyr)
fluxnames <- c('FCH4','FCH4_F','FCH4_F_ANNOPTLM','FCH4_F_ANNOPTLM_QC',
'NEE','NEE_F','NEE_F_ANNOPTLM','GPP_DT','GPP_NT','RECO_DT','RECO_NT','ET')
path="~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data"
files <- dir(path)
agg1<-list()
agg2<-list()
agg3<-list()
newdata<-list()
# dfl<- lapply(Sys.glob("*.csv"), read.csv)
for (i in 1:length(files)){
# read in FLUXNET CH4 data
setwd("~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data")
df <- read.csv(files[i],header=T)
colnames(df)
# some sites may miss multiple columns, check if miss it add with -9999
df[fluxnames[!(fluxnames %in% colnames(df))]] = -9999
colnames(df)
df <- data.frame(df$TIMESTAMP,df$FCH4,df$FCH4_F,df$FCH4_F_ANNOPTLM,df$FCH4_F_ANNOPTLM_QC,
df$NEE,df$NEE_F,df$NEE_F_ANNOPTLM,df$GPP_DT,df$GPP_NT,df$RECO_DT,df$RECO_NT)
# extrat site name from filename
sn1 <- data.frame(fn = files[i])
sn2 <- separate(sn1, "fn", c("FLX", "sitename", "FLUXNET-CH4"), sep = "_")
sn2
sn3 <- rep(toString(sn2[2]),nrow(df))
# add year month day columns
# https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html
year <- as.numeric(str_sub(df$df.TIMESTAMP,1,4))
month<- as.numeric(str_sub(df$df.TIMESTAMP,5,6))
day<- as.numeric(str_sub(df$df.TIMESTAMP,7,8))
df<-data.frame("TIMESTAMP"=df[,1],"sitename"=sn3,year,month,day,df[,-1])
# replace -9999 with NA
df <- replace(df, df == -9999, NA)
colnames(df)[6:16]<-fluxnames
colnames(df)
# convert unit from nmol/m2/s to gC/m2/day for CH4, from umol/m2/s to gC/m2/day for NEE, GPP, ER
df[,6:8]<-df[,6:8]*1e-9*12*3600*24 # factor of ~1.037
df[,10:16]<-df[,10:16]*1e-6*12*3600*24
# aggregate daily to monthly, if NA value exist, the whole month count as NA (na.rm=F)
agg1[[i]] <- aggregate(df[,c(6:16)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=F)
# aggregate daily to monthly, take average no matter how many NA days exist (na.rm=T)
agg2[[i]] <- aggregate(df[,c(6:16)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=T)
# aggreate daily to monthly, if NA value >=20 days, the whole month count as NA
my_fun <- function(x) sum(is.na(x))
agg3[[i]] <- aggregate(df, by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=my_fun)
colnames(agg3[[i]])[9:19] <- paste(fluxnames,'_NAcount',sep='')
# now agg2 have averaged values while agg3 decide if agg2 values can be kept depending on NAcount
# if NAcount<20, keep the value for that month, otherwise replace with NA
newdata[[i]]<-cbind(agg2[[i]],agg3[[i]][,9:19])
newdata[[i]][,4:14][newdata[[i]][,15:25]>20] <- NA
# d$Var2[d$Var1 == "C"] <- "Medium"
# it's eaier for matlab readtable function if save NA into -9999, so:
newdata[[i]][is.na(newdata[[i]])]<- -9999
# due to the length of CBF drier file, FLUXNET data are cut after year 2016
newdata[[i]] <- newdata[[i]][!newdata[[i]][,3]>2021,]
setwd(paste('~/RESEARCH/WORKFLOW/JCR_FLUXNET_2021_stage2/S1/',sep=''))
# write.csv(newdata[[i]], file = paste(i,'_',toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
write.csv(newdata[[i]], file = paste(toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
}
# # combine the list of dataframes into one
# ann <- bind_rows(agg1, .id = "column_label")
#  units are mgC/m2/day for CH4, g for c02
library(stringr)
library(tidyr)
library(dplyr)
fluxnames <- c('FCH4','FCH4_F','FCH4_F_ANNOPTLM','FCH4_F_ANNOPTLM_QC',
'NEE','NEE_F','NEE_F_ANNOPTLM','GPP_DT','GPP_NT','RECO_DT','RECO_NT','ET')
path="~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data"
files <- dir(path)
agg1<-list()
agg2<-list()
agg3<-list()
newdata<-list()
# dfl<- lapply(Sys.glob("*.csv"), read.csv)
i=1
setwd("~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data")
df <- read.csv(files[i],header=T)
colnames(df)
# some sites may miss multiple columns, check if miss it add with -9999
df[fluxnames[!(fluxnames %in% colnames(df))]] = -9999
colnames(df)
df <- data.frame(df$TIMESTAMP,df$FCH4,df$FCH4_F,df$FCH4_F_ANNOPTLM,df$FCH4_F_ANNOPTLM_QC,
df$NEE,df$NEE_F,df$NEE_F_ANNOPTLM,df$GPP_DT,df$GPP_NT,df$RECO_DT,df$RECO_NT)
# extrat site name from filename
sn1 <- data.frame(fn = files[i])
sn2 <- separate(sn1, "fn", c("FLX", "sitename", "FLUXNET-CH4"), sep = "_")
sn2
sn3 <- rep(toString(sn2[2]),nrow(df))
# add year month day columns
# https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html
year <- as.numeric(str_sub(df$df.TIMESTAMP,1,4))
month<- as.numeric(str_sub(df$df.TIMESTAMP,5,6))
day<- as.numeric(str_sub(df$df.TIMESTAMP,7,8))
df<-data.frame("TIMESTAMP"=df[,1],"sitename"=sn3,year,month,day,df[,-1])
# replace -9999 with NA
df <- replace(df, df == -9999, NA)
colnames(df)[6:16]<-fluxnames
colnames(df)
fluxnames
#  units are mgC/m2/day for CH4, g for c02
library(stringr)
library(tidyr)
library(dplyr)
fluxnames <- c('FCH4','FCH4_F','FCH4_F_ANNOPTLM','FCH4_F_ANNOPTLM_QC',
'NEE','NEE_F','NEE_F_ANNOPTLM','GPP_DT','GPP_NT','RECO_DT','RECO_NT','ET')
path="~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data"
files <- dir(path)
agg1<-list()
agg2<-list()
agg3<-list()
newdata<-list()
# dfl<- lapply(Sys.glob("*.csv"), read.csv)
i=1
# read in FLUXNET CH4 data
setwd("~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data")
df <- read.csv(files[i],header=T)
colnames(df)
# some sites may miss multiple columns, check if miss it add with -9999
df[fluxnames[!(fluxnames %in% colnames(df))]] = -9999
colnames(df)
df <- data.frame(df$TIMESTAMP,df$FCH4,df$FCH4_F,df$FCH4_F_ANNOPTLM,df$FCH4_F_ANNOPTLM_QC,
df$NEE,df$NEE_F,df$NEE_F_ANNOPTLM,df$GPP_DT,df$GPP_NT,df$RECO_DT,df$RECO_NT)
# extrat site name from filename
sn1 <- data.frame(fn = files[i])
sn2 <- separate(sn1, "fn", c("FLX", "sitename", "FLUXNET-CH4"), sep = "_")
sn2
sn3 <- rep(toString(sn2[2]),nrow(df))
# add year month day columns
# https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html
year <- as.numeric(str_sub(df$df.TIMESTAMP,1,4))
month<- as.numeric(str_sub(df$df.TIMESTAMP,5,6))
day<- as.numeric(str_sub(df$df.TIMESTAMP,7,8))
df<-data.frame("TIMESTAMP"=df[,1],"sitename"=sn3,year,month,day,df[,-1])
# replace -9999 with NA
df <- replace(df, df == -9999, NA)
View(df)
colnames(df)[6:16]
fluxnames
dim(df)
files(1)
files
files[1]
#  units are mgC/m2/day for CH4, g for c02
library(stringr)
library(tidyr)
library(dplyr)
fluxnames <- c('FCH4','FCH4_F','FCH4_F_ANNOPTLM','FCH4_F_ANNOPTLM_QC',
'NEE','NEE_F','NEE_F_ANNOPTLM','GPP_DT','GPP_NT','RECO_DT','RECO_NT','LE','LE_F')
# 'LE' is orginal data, 'LE_F' is gap-filled data https://fluxnet.org/data/fluxnet-ch4-community-product/data-variables/
path="~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data"
files <- dir(path)
agg1<-list()
agg2<-list()
agg3<-list()
newdata<-list()
# dfl<- lapply(Sys.glob("*.csv"), read.csv)
i=1
# read in FLUXNET CH4 data
setwd("~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data")
df <- read.csv(files[i],header=T)
colnames(df)
# some sites may miss multiple columns, check if miss it add with -9999
df[fluxnames[!(fluxnames %in% colnames(df))]] = -9999
colnames(df)
respired_fraction
# read in FLUXNET CH4 data
setwd("~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data")
df <- read.csv(files[i],header=T)
colnames(df)
# some sites may miss multiple columns, check if miss it add with -9999
df[fluxnames[!(fluxnames %in% colnames(df))]] = -9999
colnames(df)
df <- data.frame(df$TIMESTAMP,df$FCH4,df$FCH4_F,df$FCH4_F_ANNOPTLM,df$FCH4_F_ANNOPTLM_QC,
df$NEE,df$NEE_F,df$NEE_F_ANNOPTLM,df$GPP_DT,df$GPP_NT,df$RECO_DT,df$RECO_NT,df$LE,df$LE_F,df$LE_F_ANNOPTLM)
# extrat site name from filename
sn1 <- data.frame(fn = files[i])
sn2 <- separate(sn1, "fn", c("FLX", "sitename", "FLUXNET-CH4"), sep = "_")
sn2
sn3 <- rep(toString(sn2[2]),nrow(df))
# add year month day columns
# https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html
year <- as.numeric(str_sub(df$df.TIMESTAMP,1,4))
month<- as.numeric(str_sub(df$df.TIMESTAMP,5,6))
day<- as.numeric(str_sub(df$df.TIMESTAMP,7,8))
df<-data.frame("TIMESTAMP"=df[,1],"sitename"=sn3,year,month,day,df[,-1])
fluxnames
fluxnames <- c('FCH4','FCH4_F','FCH4_F_ANNOPTLM','FCH4_F_ANNOPTLM_QC',
'NEE','NEE_F','NEE_F_ANNOPTLM','GPP_DT','GPP_NT','RECO_DT','RECO_NT','LE','LE_F','LE_F_ANNOPTLM')
colnames(df)
df <- replace(df, df == -9999, NA)
colnames(df)[6:19]<-fluxnames
colnames(df)
df[,6:8]<-df[,6:8]*1e-9*12*3600*24 # factor of ~1.037
df[,10:16]<-df[,10:16]*1e-6*12*3600*24
# convert LE (W/m2) to ET mm/day: ET=LE/water_latent_heat_of_vaporization
# water_latent_heat_of_vaporization 2264.705 kJ/kg
# ET (mm/day) = LE (W/m2) *3600*24/(2264.705)*0.001   # s to day and then kg to m3
df[,17:19]<-df[,17:19]*3600*24/(2264.705)*0.001
# aggregate daily to monthly, if NA value exist, the whole month count as NA (na.rm=F)
agg1[[i]] <- aggregate(df[,c(6:19)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=F)
agg2[[i]] <- aggregate(df[,c(6:19)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=T)
aggreate daily to monthly, if NA value >=20 days, the whole month count as NA
my_fun <- function(x) sum(is.na(x))
agg3[[i]] <- aggregate(df, by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=my_fun)
# aggreate daily to monthly, if NA value >=20 days, the whole month count as NA
my_fun <- function(x) sum(is.na(x))
agg3[[i]] <- aggregate(df, by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=my_fun)
olnames(agg3)
colnames(agg3)
View(agg3)
View(agg3[[1]])
colnames(agg3[[1]])
colnames(agg3[[i]])[9:22] <- paste(fluxnames,'_NAcount',sep='')
# now agg2 have averaged values while agg3 decide if agg2 values can be kept depending on NAcount
# if NAcount<20, keep the value for that month, otherwise replace with NA
newdata[[i]]<-cbind(agg2[[i]],agg3[[i]][,9:22])
newdata[[i]]
View(newdata)
View(newdata)
colnames(newdata[[i]][,15:25])
colnames(agg1[[i]])
colnames(agg2[[i]])
colnames(agg3[[i]])
colnames(newdata[[i]][,18:25])
colnames(newdata[[i]][,18:28])
colnames(newdata[[i]][,18:31])
colnames(newdata[[i]])
my_fun <- function(x) sum(is.na(x))
agg3[[i]] <- aggregate(df, by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=my_fun)
colnames(agg3[[i]])[9:22] <- paste(fluxnames,'_NAcount',sep='')
# now agg2 have averaged values while agg3 decide if agg2 values can be kept depending on NAcount
# if NAcount<20, keep the value for that month, otherwise replace with NA
newdata[[i]]<-cbind(agg2[[i]],agg3[[i]][,9:22])
newdata[[i]][,4:17][newdata[[i]][,18:31]>20] <- NA
# it's eaier for matlab readtable function if save NA into -9999, so:
newdata[[i]][is.na(newdata[[i]])]<- -9999
# due to the length of CBF drier file, FLUXNET data are cut after year 2016
newdata[[i]] <- newdata[[i]][!newdata[[i]][,3]>2021,]
setwd(paste('~/RESEARCH/WORKFLOW/JCR_FLUXNET_2021_stage2/S1/',sep=''))
# write.csv(newdata[[i]], file = paste(i,'_',toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
write.csv(newdata[[i]], file = paste(toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
#  units are mgC/m2/day for CH4, g for c02
library(stringr)
library(tidyr)
library(dplyr)
fluxnames <- c('FCH4','FCH4_F','FCH4_F_ANNOPTLM','FCH4_F_ANNOPTLM_QC',
'NEE','NEE_F','NEE_F_ANNOPTLM','GPP_DT','GPP_NT','RECO_DT','RECO_NT','LE','LE_F','LE_F_ANNOPTLM')
# 'LE' is orginal data, 'LE_F' is gap-filled data https://fluxnet.org/data/fluxnet-ch4-community-product/data-variables/
path="~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data"
files <- dir(path)
agg1<-list()
agg2<-list()
agg3<-list()
newdata<-list()
# dfl<- lapply(Sys.glob("*.csv"), read.csv)
for (i in 1:length(files)){
# read in FLUXNET CH4 data
setwd("~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data")
df <- read.csv(files[i],header=T)
colnames(df)
# some sites may miss multiple columns, check if miss it add with -9999
df[fluxnames[!(fluxnames %in% colnames(df))]] = -9999
colnames(df)
df <- data.frame(df$TIMESTAMP,df$FCH4,df$FCH4_F,df$FCH4_F_ANNOPTLM,df$FCH4_F_ANNOPTLM_QC,
df$NEE,df$NEE_F,df$NEE_F_ANNOPTLM,df$GPP_DT,df$GPP_NT,df$RECO_DT,df$RECO_NT,df$LE,df$LE_F,df$LE_F_ANNOPTLM)
# extrat site name from filename
sn1 <- data.frame(fn = files[i])
sn2 <- separate(sn1, "fn", c("FLX", "sitename", "FLUXNET-CH4"), sep = "_")
sn2
sn3 <- rep(toString(sn2[2]),nrow(df))
# add year month day columns
# https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html
year <- as.numeric(str_sub(df$df.TIMESTAMP,1,4))
month<- as.numeric(str_sub(df$df.TIMESTAMP,5,6))
day<- as.numeric(str_sub(df$df.TIMESTAMP,7,8))
df<-data.frame("TIMESTAMP"=df[,1],"sitename"=sn3,year,month,day,df[,-1])
# replace -9999 with NA
df <- replace(df, df == -9999, NA)
colnames(df)[6:19]<-fluxnames
colnames(df)
# convert unit from nmol/m2/s to gC/m2/day for CH4, from umol/m2/s to gC/m2/day for NEE, GPP, ER
df[,6:8]<-df[,6:8]*1e-9*12*3600*24 # factor of ~1.037
df[,10:16]<-df[,10:16]*1e-6*12*3600*24
# convert LE (W/m2) to ET mm/day: ET=LE/water_latent_heat_of_vaporization
# water_latent_heat_of_vaporization 2264.705 kJ/kg
# ET (mm/day) = LE (W/m2) *3600*24/(2264.705)*0.001   # s to day and then kg to m3
df[,17:19]<-df[,17:19]*3600*24/(2264.705)*0.001
# aggregate daily to monthly, if NA value exist, the whole month count as NA (na.rm=F)
agg1[[i]] <- aggregate(df[,c(6:19)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=F)
# aggregate daily to monthly, take average no matter how many NA days exist (na.rm=T)
agg2[[i]] <- aggregate(df[,c(6:19)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=T)
# aggreate daily to monthly, if NA value >=20 days, the whole month count as NA
my_fun <- function(x) sum(is.na(x))
agg3[[i]] <- aggregate(df, by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=my_fun)
colnames(agg3[[i]])[9:22] <- paste(fluxnames,'_NAcount',sep='')
# now agg2 have averaged values while agg3 decide if agg2 values can be kept depending on NAcount
# if NAcount<20, keep the value for that month, otherwise replace with NA
newdata[[i]]<-cbind(agg2[[i]],agg3[[i]][,9:22])
newdata[[i]][,4:17][newdata[[i]][,18:31]>20] <- NA
# d$Var2[d$Var1 == "C"] <- "Medium"
# it's eaier for matlab readtable function if save NA into -9999, so:
newdata[[i]][is.na(newdata[[i]])]<- -9999
# due to the length of CBF drier file, FLUXNET data are cut after year 2021, ERA5 cuts at 2021
newdata[[i]] <- newdata[[i]][!newdata[[i]][,3]>2021,]
setwd(paste('~/RESEARCH/WORKFLOW/JCR_FLUXNET_2021_stage2/S1/',sep=''))
# write.csv(newdata[[i]], file = paste(i,'_',toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
write.csv(newdata[[i]], file = paste(toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
}
# # combine the list of dataframes into one
# ann <- bind_rows(agg1, .id = "column_label")
i=80
# read in FLUXNET CH4 data
setwd("~/RESEARCH/DATA/FLUXNET/2020release_CH4/Jan2021_Download/DD_data")
df <- read.csv(files[i],header=T)
colnames(df)
# some sites may miss multiple columns, check if miss it add with -9999
df[fluxnames[!(fluxnames %in% colnames(df))]] = -9999
colnames(df)
df <- data.frame(df$TIMESTAMP,df$FCH4,df$FCH4_F,df$FCH4_F_ANNOPTLM,df$FCH4_F_ANNOPTLM_QC,
df$NEE,df$NEE_F,df$NEE_F_ANNOPTLM,df$GPP_DT,df$GPP_NT,df$RECO_DT,df$RECO_NT,df$LE,df$LE_F,df$LE_F_ANNOPTLM)
# extrat site name from filename
sn1 <- data.frame(fn = files[i])
sn2 <- separate(sn1, "fn", c("FLX", "sitename", "FLUXNET-CH4"), sep = "_")
sn2
sn3 <- rep(toString(sn2[2]),nrow(df))
# add year month day columns
# https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html
year <- as.numeric(str_sub(df$df.TIMESTAMP,1,4))
month<- as.numeric(str_sub(df$df.TIMESTAMP,5,6))
day<- as.numeric(str_sub(df$df.TIMESTAMP,7,8))
df<-data.frame("TIMESTAMP"=df[,1],"sitename"=sn3,year,month,day,df[,-1])
# replace -9999 with NA
df <- replace(df, df == -9999, NA)
colnames(df)[6:19]<-fluxnames
colnames(df)
# convert unit from nmol/m2/s to gC/m2/day for CH4, from umol/m2/s to gC/m2/day for NEE, GPP, ER
df[,6:8]<-df[,6:8]*1e-9*12*3600*24 # factor of ~1.037
df[,10:16]<-df[,10:16]*1e-6*12*3600*24
# convert LE (W/m2) to ET mm/day: ET=LE/water_latent_heat_of_vaporization
# water_latent_heat_of_vaporization 2264.705 kJ/kg
# ET (mm/day) = LE (W/m2) *3600*24/(2264.705)*0.001   # s to day and then kg to m3
df[,17:19]<-df[,17:19]*3600*24/(2264.705)*0.001
# aggregate daily to monthly, if NA value exist, the whole month count as NA (na.rm=F)
agg1[[i]] <- aggregate(df[,c(6:19)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=F)
# aggregate daily to monthly, take average no matter how many NA days exist (na.rm=T)
agg2[[i]] <- aggregate(df[,c(6:19)], by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=mean, na.rm=T)
# aggreate daily to monthly, if NA value >=20 days, the whole month count as NA
my_fun <- function(x) sum(is.na(x))
agg3[[i]] <- aggregate(df, by=list(sitename=df$sitename,month=df$month,year=df$year), FUN=my_fun)
colnames(agg3[[i]])[9:22] <- paste(fluxnames,'_NAcount',sep='')
# now agg2 have averaged values while agg3 decide if agg2 values can be kept depending on NAcount
# if NAcount<20, keep the value for that month, otherwise replace with NA
newdata[[i]]<-cbind(agg2[[i]],agg3[[i]][,9:22])
newdata[[i]][,4:17][newdata[[i]][,18:31]>20] <- NA
# d$Var2[d$Var1 == "C"] <- "Medium"
# it's eaier for matlab readtable function if save NA into -9999, so:
newdata[[i]][is.na(newdata[[i]])]<- -9999
# due to the length of CBF drier file, FLUXNET data are cut after year 2021, ERA5 cuts at 2021
newdata[[i]] <- newdata[[i]][!newdata[[i]][,3]>2021,]
setwd(paste('~/RESEARCH/WORKFLOW/JCR_FLUXNET_2021_stage2/S1/',sep=''))
# write.csv(newdata[[i]], file = paste(i,'_',toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
write.csv(newdata[[i]], file = paste(toString(sn2[2]),"_obs.csv",sep=''),row.names = F)
toString(sn2[2])
